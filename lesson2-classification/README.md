## 学习率lr
lr 是学习率（learning rate）的缩写，它是一个超参数，控制着每次参数更新的步长大小。学习率对模型训练有以下影响：

- 步长大小

学习率决定了每次参数更新的幅度。较大的学习率会使参数更新幅度较大，较小的学习率会使参数更新幅度较小。

- 收敛速度

较高的学习率可能会加快模型的收敛速度，但如果学习率过高，可能会导致模型在最优解附近震荡，甚至无法收敛。
较低的学习率可能会使模型更稳定地收敛，但收敛速度会变慢，训练时间会增加。

- 训练稳定性

过高的学习率可能会导致训练过程不稳定，损失函数在训练过程中可能会出现剧烈波动。
过低的学习率可能会导致模型陷入局部最优解，无法找到全局最优解。
在你的代码中，lr=0.2 表示使用 0.2 的学习率进行梯度下降优化。这个值需要根据具体的任务和数据进行调整，以找到一个合适的平衡点，使模型能够快速且稳定地收敛。


## 损失函数Loss

### 均方误差损失（MSE Loss）
适用于回归任务。
计算预测值与真实值之间的平方差的平均值。
criterion = torch.nn.MSELoss()

### 交叉熵损失（Cross-Entropy Loss）
适用于分类任务，特别是多分类任务。
衡量预测的概率分布与真实分布之间的差异。
criterion = torch.nn.CrossEntropyLoss()

### 二元交叉熵损失（Binary Cross-Entropy Loss）
适用于二分类任务。
衡量二分类任务中预测的概率与真实标签之间的差异
criterion = torch.nn.BCELoss()

### 负对数似然损失（Negative Log-Likelihood Loss）
适用于分类任务
criterion = torch.nn.NLLLoss()

### 平滑 L1 损失（Smooth L1 Loss）
适用于回归任务，结合了 L1 损失和 L2 损失的优点。
对于小误差，使用 L2 损失；对于大误差，使用 L1 损失。
criterion = torch.nn.SmoothL1Loss()