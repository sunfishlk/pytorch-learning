神经网络中的激活函数是引入非线性的重要工具，使得神经网络能够逼近复杂的函数关系。以下是几种常用的激活函数：

### 1. **Sigmoid 函数**
   - **表达式**: \( \text{Sigmoid}(x) = \frac{1}{1 + e^{-x}} \)
   - **特点**:
     - 输出范围在 \( (0, 1) \) 之间。
     - 通常用于二分类任务的输出层。
     - 存在梯度消失问题，当输入值较大或较小时，梯度趋近于零，导致学习过程缓慢。

### 2. **Tanh 函数**
   - **表达式**: \( \text{Tanh}(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} \)
   - **特点**:
     - 输出范围在 \( (-1, 1) \) 之间。
     - 相较于 Sigmoid 函数，Tanh 函数的输出更集中于零附近，梯度更大，收敛速度更快。
     - 也存在梯度消失问题，但比 Sigmoid 函数略好。

### 3. **ReLU 函数（Rectified Linear Unit）**
   - **表达式**: \( \text{ReLU}(x) = \max(0, x) \)
   - **特点**:
     - 输出非负，若输入为负则输出为零。
     - 简单高效，计算量小，解决了部分梯度消失问题。
     - 存在“神经元死亡”问题：如果大量输入为负，神经元可能永远不会被激活，梯度为零。

### 4. **Leaky ReLU 函数**
   - **表达式**: \( \text{Leaky ReLU}(x) = \max(\alpha x, x) \)，其中 \( \alpha \) 是一个很小的常数（通常为 0.01）。
   - **特点**:
     - 类似于 ReLU，但允许负输入有一个很小的斜率，以避免“神经元死亡”问题。
     - 更稳健，但增加了计算复杂度。

### 5. **ELU 函数（Exponential Linear Unit）**
   - **表达式**: 
     \[
     \text{ELU}(x) =
     \begin{cases}
     x & \text{if } x > 0 \\
     \alpha(e^x - 1) & \text{if } x \leq 0
     \end{cases}
     \]
     其中 \( \alpha \) 是一个正数。
   - **特点**:
     - 对负输入有更平滑的曲线，相比 ReLU 更稳定。
     - 可以有负值输出，有助于激活函数的均值更接近零。

### 6. **Softmax 函数**
   - **表达式**: \( \text{Softmax}(x_i) = \frac{e^{x_i}}{\sum_{j}e^{x_j}} \)
   - **特点**:
     - 常用于多分类任务的输出层。
     - 将输出变为概率分布，所有输出的和为1。
     - 适合处理多分类问题，但对梯度较为敏感。

### 7. **Swish 函数**
   - **表达式**: \( \text{Swish}(x) = x \cdot \text{Sigmoid}(x) \)
   - **特点**:
     - 由谷歌提出，结合了 ReLU 和 Sigmoid 的优点。
     - 在一些深度学习任务中表现优于 ReLU。

这些激活函数在不同的场景中都有各自的优势和适用性，选择合适的激活函数可以显著影响神经网络的性能和训练效果。

