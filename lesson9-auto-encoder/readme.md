## Auto Encoder

自编码器

输入图像压缩，减少输入量提高速度

自编码器（Autoencoder）是一种神经网络模型，主要用于数据的无监督学习和特征提取。自编码器的核心思想是通过将输入数据编码成一个低维或稀疏的表示，然后再从这种表示中重建原始数据。通过这种方式，自编码器可以学习到数据的主要特征或模式。

自编码器的结构一般包括三个部分：

1. **编码器（Encoder）**：将输入数据压缩到一个低维的潜在空间表示（latent space）。这一部分通常由几层神经网络组成，逐步减少数据的维度。

2. **潜在空间表示（Latent Space Representation）**：编码器输出的低维表示，这是一种压缩后的数据形式，保留了输入数据的主要特征。

3. **解码器（Decoder）**：从潜在空间表示重建原始数据。解码器的结构通常与编码器对称，通过逐步恢复数据的维度，尽量还原出与输入数据相似的输出。

自编码器的训练目标是使输入数据与重建的数据尽可能接近，即最小化输入与输出之间的差异（通常使用均方误差作为损失函数）。通过这种方式，自编码器可以学习到数据中的重要模式和特征。

自编码器有许多变种和应用，包括但不限于：

- **去噪自编码器（Denoising Autoencoder）**：训练时向输入数据添加噪声，然后要求网络恢复出无噪声的原始数据，用于去噪任务。
- **稀疏自编码器（Sparse Autoencoder）**：在潜在表示中加入稀疏性约束，以使得表示更加紧凑、提取特征更加有效。
- **变分自编码器（Variational Autoencoder, VAE）**：在潜在空间上加入概率分布，广泛应用于生成模型。

自编码器广泛应用于数据降维、特征学习、异常检测和图像生成等领域。

通过将原数据白色的X 压缩, 解压 成黑色的X, 然后通过对比黑白 X ,求出预测误差, 进行反向传递, 逐步提升自编码的准确性

从头到尾, 我们只用到了输入数据 X, 并没有用到 X 对应的数据标签, 所以也可以说自编码是一种非监督学习

自编码器用于数据的特征提取，得到数据的精髓，减少神经网络的训练输入负担，同样能达到很好的效果

