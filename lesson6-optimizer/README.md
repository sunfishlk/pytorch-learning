# 优化器Optimizer

### 随机梯度下降SGD(Stochastic Gradient Descent)

### 权重Momentum

### AdaGrad

### RMSProp

### Adam

SGD 是最普通的优化器, 也可以说没有加速效果, 而 Momentum 是 SGD 的改良版, 它加入了动量原则. 后面的 RMSprop 又是 Momentum 的升级版. 而 Adam 又是 RMSprop 的升级版. 